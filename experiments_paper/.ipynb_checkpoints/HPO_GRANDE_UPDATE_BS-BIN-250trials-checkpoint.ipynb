{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c3b444-064b-4639-983d-238b3da68ad9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6531aaa-2c26-40f5-8078-d754aceda153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'GRANDE': {\n",
    "        'split_index_activation': 'entmax',  \n",
    "        'normalize_index_array': False,\n",
    "        'split_decision_activation': 'softsign', # sigmoid | softsign\n",
    "        \n",
    "        'depth': 6,\n",
    "        'n_estimators': 2048,\n",
    "                \n",
    "        'learning_rate_weights': 0.005,\n",
    "        'learning_rate_index': 0.01,\n",
    "        'learning_rate_values': 0.01,\n",
    "        'learning_rate_leaf': 0.01,   \n",
    "\n",
    "        'fine_tune': False,\n",
    "        'cosine_decay_steps': 0,        \n",
    "        \n",
    "        'reduce_lr': False,\n",
    "        'reduce_lr_each_eval_nums': 10,\n",
    "        'reduce_lr_fraction': 0.2,\n",
    "        \n",
    "        'dropout': 0,#0.25,\n",
    "        \n",
    "        'selected_variables': 0.8, #int number or float fraction\n",
    "        'data_subset_fraction': 1.0,\n",
    "        'bootstrap': False,\n",
    "                \n",
    "        'initializer': 'RandomNormal', \n",
    "        \n",
    "        'optimizer': 'SWA', \n",
    "        \n",
    "        'epochs': 10_000,\n",
    "        'batch_size': 64,#512\n",
    "        'drop_remainder': True,\n",
    "        \n",
    "        \n",
    "        'loss': 'focal_crossentropy', \n",
    "        'focalLossGamma': 2.0,\n",
    "        \n",
    "        'transformation_type': 'mean', #for regression; mean log       \n",
    "        \n",
    "        'polyLoss': False,\n",
    "        'polyLossEpsilon': 2.0,\n",
    "        \n",
    "        'weight_activation_st': 'softmax',\n",
    "        'estimator_leaf_weights': True,\n",
    "\n",
    "        'modify_grads': False,\n",
    "        \n",
    "        'logit_weights': False,\n",
    "        #'weight_freeze_steps': 0,\n",
    "        'warmup_steps': 0,\n",
    "        \n",
    "        'restarts': 0,\n",
    "        'restart_type': 'loss', #'loss', 'metric'\n",
    "        \n",
    "        'early_stopping_epochs': 25,\n",
    "        'early_stopping_type': 'loss', #'loss', 'metric'\n",
    "        'early_stopping_epsilon': 1e-3,\n",
    "        \n",
    "        'class_weights': True,\n",
    "        'seed_model': 1,\n",
    "    },\n",
    "    \n",
    "    'preprocessing': {\n",
    "        'balance_threshold': 0,#.25, #if minclass fraction less than threshold/num_classes | #0=no rebalance, 1=rebalance all\n",
    "        'normalization_technique': 'quantile', #'min-max'\n",
    "        'encoding_type': 'LOO', #GLMM\n",
    "        'one_hot_encode_threshold': 10,\n",
    "        'quantile_noise': 1e-3,\n",
    "\n",
    "        'XGBoostEncoding': False,\n",
    "        'CatBoostEncoding': True,\n",
    "    },\n",
    "\n",
    "    'computation': {\n",
    "        'random_seed': 42,\n",
    "        'cv_num_eval': 5,\n",
    "        'subset_size': None, # medium: 10_000, large: 50_000, keep normal size: None\n",
    "        \n",
    "        'use_best_hpo_result': True,# True,\n",
    "        'hpo_path': '_HPO_250_BS-2x5',\n",
    "        \n",
    "        'force_depth': False,\n",
    "        'force_dropout': False,\n",
    "        'force_class_weights': False,\n",
    "        'force_restart': True,\n",
    "        \n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '0,1,2,3',\n",
    "        \n",
    "        'parallel_jobs_cv': 1,\n",
    "        'jobs_per_gpu': 2,\n",
    "                \n",
    "        'search_iterations': 60,\n",
    "        'cv_num_hpo': 2,     \n",
    "        'report_test_performance': False,\n",
    "        \n",
    "        'metrics_class': ['f1', 'balanced_accuracy', 'roc_auc'],\n",
    "        'metrics_reg': ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "        \n",
    "        'eval_metric_class': ['f1', 'balanced_accuracy', 'roc_auc'],\n",
    "        'eval_metric_reg': ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "\n",
    "        'max_hours': 2.0,   \n",
    "\n",
    "        'evaluate_GRANDE': True,\n",
    "    },\n",
    "    \n",
    "    'benchmarks': {          \n",
    "        'CART': False,\n",
    "        'RandomForest': False,\n",
    "        \n",
    "        'XGB': True,\n",
    "        'CatBoost': True,\n",
    "        'NODE': True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab8be2-1b4c-4370-a837-9176d15bea41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if config['computation']['use_gpu']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config['computation']['gpu_numbers'])\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ['XLA_FLAGS'] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.6\"\n",
    "    os.environ['TF_XLA_FLAGS'] = \"--tf_xla_enable_xla_devices --tf_xla_auto_jit=2\"    \n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:512\"    \n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false' \n",
    "#os.environ['TF_XLA_FLAGS'] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"worker\").setLevel(logging.ERROR)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utilities.utilities_GRANDE import *\n",
    "from utilities.GRANDE import *\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "import joblib\n",
    "import ray\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()\n",
    "from ray.autoscaler.sdk import request_resources\n",
    "ray.init()\n",
    "\n",
    "tf.random.set_seed(config['computation']['random_seed'])\n",
    "np.random.seed(config['computation']['random_seed'])\n",
    "random.seed(config['computation']['random_seed'])\n",
    "\n",
    "from datetime import datetime\n",
    "timestr = datetime.utcnow().strftime('%Y-%m-%d--%H-%M-%S%f')\n",
    "print(timestr)\n",
    "os.makedirs(os.path.dirname(\"./evaluation_results\" + config['computation']['hpo_path'] + \"/latex_tables/\" + timestr +\"/\"), exist_ok=True)\n",
    "\n",
    "filepath = './evaluation_results' + config['computation']['hpo_path'] + '/depth' + str(config['GRANDE']['depth']) + '/' + timestr + '/'\n",
    "Path(filepath).mkdir(parents=True, exist_ok=True)     \n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc5b5a-5142-441a-89c0-5eb42f6e0be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))\n",
    "config['computation']['number_of_gpus'] = len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c310a2-86c7-43fb-9ba1-4ec99aae37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parallel CV Jobs Nested:', config['computation']['parallel_jobs_cv'])\n",
    "print('Parallel Jobs per GPU:', config['computation']['jobs_per_gpu'])\n",
    "\n",
    "config['computation']['n_jobs'] = config['computation']['jobs_per_gpu']*config['computation']['parallel_jobs_cv']\n",
    "config['computation']['num_gpu'] = 1/(np.ceil(config['computation']['n_jobs']/config['computation']['number_of_gpus']))      \n",
    "\n",
    "config['computation']['verbose'] = 0\n",
    "n_jobs_outer = max(1, int(np.floor(config['computation']['number_of_gpus'] / 2)))\n",
    "config['computation']['number_of_gpus'] = config['computation']['number_of_gpus'] / n_jobs_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e73582-0f9e-4522-9a94-dc59f4e566e9",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7818a1-50fa-4654-a40e-05bb6136db38",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db60dc5-8da4-4f6e-9ec1-eb3f886555db",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict_GRANDE = {\n",
    "    \n",
    "        'depth': tune.choice([4,6]),#tune.randint(5,9),#tune.randint(2,8),\n",
    "        'n_estimators': tune.choice([512,1024,2048]),#tune.randint(512, 1024),#tune.randint(128, 2048),\n",
    "    \n",
    "    \n",
    "        'learning_rate_weights': tune.loguniform(0.0001, 0.1),\n",
    "        'learning_rate_index': tune.loguniform(0.005, 0.2),\n",
    "        'learning_rate_values': tune.loguniform(0.005, 0.2),\n",
    "        'learning_rate_leaf': tune.loguniform(0.005, 0.2),\n",
    "\n",
    "        'cosine_decay_steps': tune.choice([0, 100, 1000]),\n",
    "    \n",
    "        'loss_reg': tune.choice(['mse']),\n",
    "        'loss_class': tune.choice(['crossentropy', 'focal_crossentropy']),\n",
    "    \n",
    "        'dropout': tune.choice([0.00, 0.25, 0.5]),#tune.uniform(0.0, 0.25),\n",
    "        'selected_variables': tune.uniform(0.5, 1.0),\n",
    "        'data_subset_fraction': tune.uniform(0.8, 1.0),\n",
    "        \n",
    "        'focalLossGamma': tune.choice([3]),#tune.uniform(1.0, 10.0),\n",
    "    \n",
    "        #'polyLoss': tune.choice([True, False]),\n",
    "        #'polyLossEpsilon': tune.choice([-1, 1, 5, 10, 50]),#tune.uniform(1.0, 100.0),\n",
    "    \n",
    "        'optimizer': tune.choice(['SWA']), #'adam', 'SWA', 'GradientAccumulator'\n",
    "    \n",
    "        'initializer': tune.choice(['RandomNormal']),\n",
    "    \n",
    "        'estimator_leaf_weights': tune.choice([True]),\n",
    " \n",
    "        #'reduce_lr': tune.choice([True, False]),\n",
    "        #'reduce_lr_each_eval_nums': tune.choice([config['GRANDE']['early_stopping_eval_nums']//2, config['GRANDE']['early_stopping_eval_nums']//3, config['GRANDE']['early_stopping_eval_nums']//4]),\n",
    "        #'reduce_lr_fraction': tune.choice([0.1, 0.2, 0.5]),\n",
    "    \n",
    "        #'warmup_steps': tune.choice([10, 50, 100, 500]),\n",
    "    \n",
    "        #'split_index_activation': tune.choice(['softmax', 'entmax', 'sparsemax']), \n",
    "        \n",
    "        #'logit_weights': tune.choice([True, False]),\n",
    "        'weight_activation_st': tune.choice(['softmax']),\n",
    "        #'weight_freeze_steps': tune.randint(0, 100),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7625a-a1d8-4cb1-a71f-2b44498ae2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict_CART = {\n",
    "        'max_depth': tune.randint(2, 12),\n",
    "        'random_state': tune.choice([config['computation']['random_seed']]),\n",
    "        'criterion': tune.choice(['gini', 'entropy']), \n",
    "        'min_samples_split': tune.randint(2, 50),\n",
    "        'min_samples_leaf': tune.randint(1, 50),\n",
    "\n",
    "        'ccp_alpha': tune.loguniform(1e-8, 1.0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc112c83-208a-40f6-a541-ca12f3ab1e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_dict_XGB = {\n",
    "        'learning_rate': tune.loguniform(0.01, 0.3), #TabSurvey\n",
    "        'max_depth': tune.randint(2, 12), #TabSurvey\n",
    "    \n",
    "        'reg_alpha': tune.loguniform(1e-8, 1.0), #TabSurvey\n",
    "        'reg_lambda':  tune.loguniform(1e-8, 1.0), #TabSurvey\n",
    "    \n",
    "        'n_estimators': tune.choice([1_000]),\n",
    "        'early_stopping_rounds': tune.choice([20]),    \n",
    "    \n",
    "        'max_cat_to_onehot': tune.choice([10]),    \n",
    "    \n",
    "        #'learning_rate': tune.loguniform(1e-5, 0.7), #TabularBenchmarks \n",
    "        #'max_depth': tune.randint(2, 12), #TabularBenchmarks\n",
    "        #'gamma': tune.loguniform(1e-8, 7.0), #TabularBenchmarks\n",
    "        #'max_depth': tune.lograndint(1, 100), #TabularBenchmarks\n",
    "    \n",
    "        #'subsample': tune.uniform(0.5, 1.0), #TabularBenchmarks\n",
    "        #'colsample_bytree': tune.uniform(0.5, 1.0), #TabularBenchmarks \n",
    "        #'colsample_bylevel': tune.uniform(0.5, 1.0), #TabularBenchmarks \n",
    "    \n",
    "        #'reg_alpha': tune.loguniform(1e-8, 1e2), #TabularBenchmarks \n",
    "        #'reg_lambda':  tune.loguniform(1.0, 4.0), #TabularBenchmarks \n",
    "\n",
    "        #'min_child_weight':  tune.quniform(1, 100, 1), #TabularBenchmarks         \n",
    "    \n",
    "        'max_cat_threshold': tune.choice([8]),    \n",
    "    \n",
    "        'n_jobs': tune.choice([1]),\n",
    "        'random_state': tune.choice([config['computation']['random_seed']]),\n",
    "        'verbosity': tune.choice([0]),\n",
    "    \n",
    "}   \n",
    "\n",
    "parameter_dict_CatBoost = {\n",
    "        'learning_rate': tune.loguniform(0.01, 0.3),#TabSurvey\n",
    "        'max_depth': tune.randint(2, 12),#TabSurvey\n",
    "        'l2_leaf_reg': tune.loguniform(0.5, 30.0),#TabSurvey\n",
    "    \n",
    "        'n_estimators': tune.choice([1_000]),\n",
    "        'early_stopping_rounds': tune.choice([20]),\n",
    "\n",
    "        'random_seed': tune.choice([config['computation']['random_seed']]),\n",
    "        'verbose': tune.choice([0]),\n",
    "    \n",
    "        'gpu_ram_part': tune.choice([0.7/config['computation']['n_jobs']/n_jobs_outer]),\n",
    "        'used_ram_limit': tune.choice(['16GB']),\n",
    "        'thread_count': tune.choice([1]),\n",
    "    \n",
    "        'boosting_type': tune.choice(['Plain']),\n",
    "        #'one_hot_max_size': tune.choice([64]),   \n",
    "        #'leaf_estimation_iterations': tune.choice([1]),   \n",
    "        #'border_count': tune.choice([256]),  \n",
    "    \n",
    "        'devices': tune.choice(['0']),\n",
    "        #'task_type' 'GPU' / 'CPU'\n",
    "}   \n",
    "\n",
    "parameter_dict_RandomForest = {\n",
    "        'n_estimators': tune.randint(5, 100), #TabSurvey \n",
    "    \n",
    "        'max_depth': tune.randint(2, 12), #TabSurvey \n",
    "        'criterion': tune.choice(['gini', 'entropy']), #TabSurvey \n",
    "\n",
    "        'n_jobs': tune.choice([1]),\n",
    "    \n",
    "        'random_state': tune.choice([config['computation']['random_seed']]),\n",
    "}   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7d795-5dcc-4c27-a048-e70c6034ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict_NODE = {\n",
    "        'num_layers': tune.choice([2,4,8]),#TabSurvey\n",
    "        'total_tree_count': tune.choice([1024, 2048]),#TabSurvey\n",
    "        'tree_depth': tune.choice([6,8]),#TabSurvey\n",
    "        #'tree_output_dim': tune.randint(2,3),#TabSurvey\n",
    "    \n",
    "        'seed': tune.choice([config['computation']['random_seed']]),\n",
    "}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01650c-0cdf-4bc1-97b4-1986c9985fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CC18 Datasets\n",
    "#https://github.com/openml/benchmark-suites/blob/44bb11328ef4ae79c67e02ffaf179bd633a5b20b/OpenML%20Benchmark%20generator.ipynb#L1038\n",
    "\n",
    "identifier_list = [\n",
    "                        'BIN:CC18_wilt', #40983                                                4,839 |      5 \n",
    "                        'BIN:CC18_phoneme', #1489                                              5,404 |      5        \n",
    "                        'BIN:CC18_Amazon_employee_access', #4135                              32,769 |      9              \n",
    "                        'BIN:CC18_ilpd', #1480                                                   583 |     10 \n",
    "                        'BIN:CC18_dresses-sales', #23381                                         500 |     12\n",
    "                        'BIN:CC18_adult', #1590                                               48,842 |     14\n",
    "                        'BIN:CC18_climate-model-simulation-crashes', #40994                      540 |     18                   \n",
    "                        'BIN:CC18_churn', #40701                                               5,000 |     20 \n",
    "                        'BIN:CC18_numerai28.6', #23517                                        96,320 |     21\n",
    "                        'BIN:CC18_wdbc', #1510                                                   569 |     30 \n",
    "                        'BIN:CC18_PhishingWebsites', #4534                                    11,055 |     30     \n",
    "                        'BIN:CC18_cylinder-bands', #6332                                         540 |     37\n",
    "                        'BIN:CC18_qsar-biodeg', #1494                                          1,055 |     41\n",
    "                        'BIN:CC18_tokyo1', #40705                                                959 |     44 \n",
    "                        'BIN:CC18_ozone-level-8hr', #1487                                      2,534 |     72           \n",
    "                        'BIN:CC18_nomao', #1486                                               34,465 |    118\n",
    "                        'BIN:CC18_SpeedDating', #40536                                         8,378 |    120\n",
    "                        'BIN:CC18_madelon', #1485                                              2,600 |    500 \n",
    "                        'BIN:CC18_Bioresponse', #4134                                          3,751 |  1,776                        \n",
    "                  ]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f794dc9-44da-4b35-be0c-acfc8394dd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective = 'regression' if 'REG:' in identifier_list[0] else 'classification'\n",
    "parameter_dict_GRANDE['objective'] = tune.choice([objective])\n",
    "\n",
    "if objective == 'classification':\n",
    "    config['computation']['eval_metric'] = config['computation']['eval_metric_class'][:1]\n",
    "    parameter_dict_GRANDE['loss'] = parameter_dict_GRANDE['loss_class']\n",
    "else:\n",
    "    config['computation']['eval_metric'] = config['computation']['eval_metric_reg'][:1]\n",
    "    parameter_dict_GRANDE['loss'] = parameter_dict_GRANDE['loss_reg']\n",
    "    del parameter_dict_RandomForest['criterion']\n",
    "    del parameter_dict_CART['criterion']\n",
    "    \n",
    "del parameter_dict_GRANDE['loss_reg']\n",
    "del parameter_dict_GRANDE['loss_class']\n",
    "\n",
    "#display(parameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ee3d9-3373-4a63-a879-c5f26b4ac8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hpo_for_dataset(identifier, \n",
    "                            objective,\n",
    "                            timestr,\n",
    "                            config, \n",
    "                            parallel_jobs_cv,\n",
    "                            jobs_per_gpu,\n",
    "                            parameter_dict_GRANDE,\n",
    "                            parameter_dict_CART,\n",
    "                            parameter_dict_GeneticTree,\n",
    "                            parameter_dict_DNDT,\n",
    "                            parameter_dict_XGB,\n",
    "                            parameter_dict_CatBoost,\n",
    "                            parameter_dict_GradientBoostingTree,\n",
    "                            parameter_dict_LightGBM,\n",
    "                            parameter_dict_RandomForest,\n",
    "                            parameter_dict_NODE):\n",
    "  \n",
    "\n",
    "    from copy import deepcopy\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.search.optuna import OptunaSearch    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from ray.tune import JupyterNotebookReporter\n",
    "    from utilities.utilities_GRANDE import get_preprocessed_dataset, hpo_GRANDE, hpo_CART, hpo_GeneticTree, hpo_DNDT, hpo_XGB, hpo_CatBoost, hpo_GradientBoostingTree, hpo_LightGBM, hpo_RandomForest, hpo_NODE, flatten_list\n",
    "    import contextlib\n",
    "\n",
    "    config_hpo = deepcopy(config)\n",
    "    \n",
    "    hpo_results_real_world_train = {}\n",
    "    hpo_results_real_world_unsorted_train = {}\n",
    "    hpo_results_real_world_valid = {}\n",
    "    hpo_results_real_world_unsorted_valid = {}\n",
    "    hpo_results_real_world_test = {}\n",
    "    hpo_results_real_world_unsorted_test = {}\n",
    "    hpo_results_real_world_cv = {}\n",
    "    hpo_results_real_world_unsorted_cv = {}\n",
    "\n",
    "    hpo_scores_dict_by_dataset = {}\n",
    "    hpo_scores_dict_by_dataset_max = {}\n",
    "\n",
    "    scores_df_by_dataset = {}    \n",
    "    \n",
    "    #print('______________________________________________________________________________________________________________')\n",
    "    print('START', identifier, flush=True)\n",
    "\n",
    "    config_hpo['GRANDE']['objective'] = objective\n",
    "    \n",
    "    hpo_scores_dict_by_dataset[identifier] = {'GRANDE': {}}\n",
    "    benchmark_dict = {}\n",
    "    \n",
    "    dataset_dict_list = get_preprocessed_dataset(identifier,\n",
    "                                                 random_seed=config['computation']['random_seed'],\n",
    "                                                 config=config_hpo,\n",
    "                                                 verbosity=0,\n",
    "                                                 hpo=True)\n",
    "    \n",
    "    dataset_dict_list_ref = ray.put(dataset_dict_list)\n",
    "    \n",
    "    #print('Parallel CV Jobs Nested:', parallel_jobs_cv)\n",
    "    #print('Parallel Jobs per GPU:', jobs_per_gpu)\n",
    "    \n",
    "    num_gpu = 1/(np.ceil(config['computation']['n_jobs']/config['computation']['number_of_gpus']))      \n",
    "\n",
    "    if config['computation']['evaluate_GRANDE']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['GRANDE'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = config['computation']['n_jobs'] if config_hpo_benchmark['computation']['use_gpu'] else 5\n",
    "        config_hpo_benchmark['computation']['num_gpu'] = num_gpu*parallel_jobs_cv if config_hpo_benchmark['computation']['use_gpu'] else 0\n",
    "        benchmark_dict['GRANDE'], hpo_scores_dict_by_dataset[identifier]['GRANDE']['cv'], hpo_scores_dict_by_dataset[identifier]['GRANDE']['test'] = hpo_GRANDE(identifier, dataset_dict_list_ref, parameter_dict_GRANDE, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)\n",
    "        \n",
    "    if config['benchmarks']['CART']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['CART'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = 3\n",
    "        benchmark_dict['CART'], hpo_scores_dict_by_dataset[identifier]['CART']['cv'], hpo_scores_dict_by_dataset[identifier]['CART']['test'] = hpo_CART(identifier, dataset_dict_list_ref, parameter_dict_CART, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)\n",
    "    if config['benchmarks']['XGB']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['XGB'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = config['computation']['n_jobs'] if config_hpo_benchmark['computation']['use_gpu'] else 5\n",
    "        config_hpo_benchmark['computation']['num_gpu'] = num_gpu*parallel_jobs_cv if config_hpo_benchmark['computation']['use_gpu'] else 0\n",
    "        parameter_dict_XGB['tree_method'] = tune.choice(['gpu_hist']) if config_hpo_benchmark['computation']['use_gpu'] else tune.choice(['hist'])\n",
    "        benchmark_dict['XGB'], hpo_scores_dict_by_dataset[identifier]['XGB']['cv'], hpo_scores_dict_by_dataset[identifier]['XGB']['test'] = hpo_XGB(identifier, dataset_dict_list_ref, parameter_dict_XGB, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)\n",
    "    if config['benchmarks']['RandomForest']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['RandomForest'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = 3\n",
    "        benchmark_dict['RandomForest'], hpo_scores_dict_by_dataset[identifier]['RandomForest']['cv'], hpo_scores_dict_by_dataset[identifier]['RandomForest']['test'] = hpo_RandomForest(identifier, dataset_dict_list_ref, parameter_dict_RandomForest, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)\n",
    "    if config['benchmarks']['NODE']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['NODE'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = 1 if config_hpo_benchmark['computation']['use_gpu'] else 3#config['computation']['n_jobs'] if config_hpo_benchmark['computation']['use_gpu'] else 5\n",
    "        config_hpo_benchmark['computation']['num_gpu'] = num_gpu*parallel_jobs_cv if config_hpo_benchmark['computation']['use_gpu'] else 0\n",
    "        benchmark_dict['NODE'], hpo_scores_dict_by_dataset[identifier]['NODE']['cv'], hpo_scores_dict_by_dataset[identifier]['NODE']['test'] = hpo_NODE(identifier, dataset_dict_list_ref, parameter_dict_NODE, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)\n",
    "    if config['benchmarks']['CatBoost']:\n",
    "        hpo_scores_dict_by_dataset[identifier]['CatBoost'] = {}\n",
    "        config_hpo_benchmark = deepcopy(config_hpo)\n",
    "        #config_hpo_benchmark['computation']['use_gpu'] = False\n",
    "        config_hpo_benchmark['computation']['n_jobs'] = config['computation']['n_jobs'] if config_hpo_benchmark['computation']['use_gpu'] else 5\n",
    "        config_hpo_benchmark['computation']['num_gpu'] = min(num_gpu*parallel_jobs_cv, 3) if config_hpo_benchmark['computation']['use_gpu'] else 0\n",
    "        parameter_dict_CatBoost['task_type'] = tune.choice(['GPU']) if config_hpo_benchmark['computation']['use_gpu'] else tune.choice(['CPU'])\n",
    "        benchmark_dict['CatBoost'], hpo_scores_dict_by_dataset[identifier]['CatBoost']['cv'], hpo_scores_dict_by_dataset[identifier]['CatBoost']['test'] = hpo_CatBoost(identifier, dataset_dict_list_ref, parameter_dict_CatBoost, config_hpo_benchmark, metric=config['computation']['eval_metric'][0], greater_better = True, timestr = timestr)               \n",
    "\n",
    "    print('DONE', identifier, flush=True)\n",
    "\n",
    "    return hpo_scores_dict_by_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89bd00-c040-4fbe-9f1d-ed55baf57bf4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.devnull, \"w\") as f, contextlib.redirect_stderr(f):\n",
    "#with contextlib.redirect_stdout(sys.stdout):\n",
    "    parallel_eval_real_world_nested = Parallel(backend='loky', n_jobs = n_jobs_outer, verbose=13, pre_dispatch=n_jobs_outer) #loky #sequential multiprocessing\n",
    "    hpo_scores_dict_by_dataset = parallel_eval_real_world_nested(delayed(perform_hpo_for_dataset)(identifier, \n",
    "                                                                                    objective,\n",
    "                                                                                    timestr,\n",
    "                                                                                    config, \n",
    "                                                                                    config['computation']['parallel_jobs_cv'],\n",
    "                                                                                    config['computation']['jobs_per_gpu'],\n",
    "                                                                                    parameter_dict_GRANDE,\n",
    "                                                                                    parameter_dict_CART,\n",
    "                                                                                    parameter_dict_GeneticTree,\n",
    "                                                                                    parameter_dict_DNDT,\n",
    "                                                                                    parameter_dict_XGB,\n",
    "                                                                                    parameter_dict_CatBoost,\n",
    "                                                                                    parameter_dict_GradientBoostingTree,\n",
    "                                                                                    parameter_dict_LightGBM,                                                                               \n",
    "                                                                                    parameter_dict_RandomForest,\n",
    "                                                                                    parameter_dict_NODE) for identifier in tqdm(identifier_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c2e34-b71b-4821-bc1e-79a8a3a707fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "if config['computation']['use_gpu']:\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7644b-7061-44e9-b3ff-46763e6f36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e3709-385b-4cfd-bbd5-46bfd4f668f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb858e7-f02f-4961-b20e-0aa9f45e08ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
